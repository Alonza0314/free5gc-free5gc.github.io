# Exploring sched_ext: BPF-Powered CPU Schedulers in the Linux Kernel
The Linux kernel's scheduler is one of its most critical components, determining which tasks run on which CPUs and for how long.

But in today's complex computing environments, the default Linux scheduler doesn't always provide optimal performance for specialized workloads. This is where sched_ext (SCX) comes in – a framework that allows implementing custom CPU schedulers in BPF (Berkeley Packet Filter) and loading them dynamically. In this technical analysis.

I'll examine the architecture and implementation of SCX schedulers, with a particular focus on scx_rustland and scx_bpflandand, compared to traditional schedulers.

## What is sched_ext?
### Sched_ext

Linux kernel 6.12 introduced `sched_ext` (“extensible scheduler”) as a new scheduling class that allows pluggable CPU schedulers via eBPF

Enables implementing and dynamically loading thread schedulers written in BPF means it unlike traditional scheduler modifications that require kernel recompilation and rebooting, `sched_ext` defines a set of hook points (operations) that an eBPF-based scheduler can implement (such as picking the next task, enqueuing/dequeuing tasks, etc.)

* **BPF struct_ops**: Used to define a scheduling policy through callback functions
* **Dispatch queues** (DSQs): Used for task queuing and execution
* **Safety mechanisms**: Prevent system crashes from buggy schedulers


 scx project is a collection of `sched_ext` schedulers and tools. Schedulers in scx range from simple demonstrative policies to production-oriented ones tailored for specific use cases:
*  **scx_simple** : uses a basic FIFO or least-run-time policy
*  **scx_nest** : places tasks on high-frequency cores
*  **scx_lavd** : is optimized for gaming workloads
*  **scx_rusty** : partitions CPUs by last-level cache to improve locality
*  **scx_bpfland** : threads that block frequently (i.e. perform many voluntary context switches per second) are assumed to be interactive, and thus prioritized

Each scheduler in SCX implements the required sched_ext hooks (via eBPF programs) and can be selected at runtime. The default Linux scheduler can always be restored if needed
## Build and run sched_ext
I use [blog](https://arighi.blogspot.com/2024/04/getting-started-with-sched-ext.html) post by Andrea Righi outlines a great workflow for testing sched_ext without modifying  existing system
### Run the virtual environment and test a scheduler
First, start the virtual environment with the sched_ext kernel:
```bash
vng -vr ../linux
```
![image](https://hackmd.io/_uploads/rJrK0yQj1e.png)

Once inside the virtual environment, you can run one of the schedulers with the helper function:


```
scx ./build/scheds/c/scx_simple
```

![image](https://hackmd.io/_uploads/SyryLyXj1g.png)

## C-Based Schedulers

**scx_simple** 

`scx_simple` is a minimal scheduler that functions either as a global weighted vtime scheduler (similar to the Completely Fair Scheduler) or as a FIFO scheduler. It's designed primarily to demonstrate basic scheduling concepts 

In the code below, from `scx_simple.bpf.c`, the `.enqueue` callback handles a task that needs to be scheduled. 
1. If the scheduler is in FIFO mode (fifo_sched == true), it simply inserts the task into the shared dispatch queue without any priority sorting. 

2. If in normal (weighted vtime) mode, it retrieves the task’s current virtual time (p->scx.dsq_vtime), adjusts it so that no task gains more than one slice worth of idle credit, 

3. inserts the task into the shared queue with that virtual time as the key:

```C
void BPF_STRUCT_OPS(simple_enqueue, struct task_struct *p, u64 enq_flags) {
    stat_inc(1);  // increment global queue count
    if (fifo_sched) {
        // FIFO scheduling: enqueue to shared queue with default slice
        scx_bpf_dsq_insert(p, SHARED_DSQ, SCX_SLICE_DFL, enq_flags);
    } else {
        u64 vtime = p->scx.dsq_vtime;
        // Cap the vtime lag to one slice to prevent too much credit
        if (time_before(vtime, vtime_now - SCX_SLICE_DFL))
            vtime = vtime_now - SCX_SLICE_DFL;
        // Enqueue with a specific virtual time for fairness
        scx_bpf_dsq_insert_vtime(p, SHARED_DSQ, SCX_SLICE_DFL, vtime, enq_flags);
    }
}

```
## scx_rustland
`scx_rustland` is designed to prioritize interactive workloads over CPU-intensive background workloads.

In practical terms, this likely means it keeps track of each task’s recent CPU usage or blocking behavior and assigns higher priority (sooner scheduling, more CPU time) to tasks that have shorter CPU bursts.
### scx_rustland overiew
`scx_rustland` uses a hybrid approach that splits functionality between kernel space and user space:

```
User Space: Complex scheduling logic in Rust
    - Task prioritization
    - CPU selection algorithms
    - Complex data structures
    |
    | Ring buffer communication
    v
Kernel Space: Minimal BPF component
    - Task state tracking
    - Dispatch queue management
    - Safety mechanisms
```

**BPF Dispatcher (scx_rustland_core)**: The BPF part of scx_rustland implements minimal logic required to interface with the kernel. Its enqueue hook, for example, does not directly decide a run queue as a normal scheduler would. Instead, it may place the task into a BPF queue map that represents “tasks waiting for user-space decision.” 

**User-Space Scheduler** (**Rust**): On the user side, the Rust scheduler process uses libraries (like libbpf or Aya in Rust) to interact with the eBPF program. It attaches to the maps exposed by BPF. Typically, it might use a ring buffer to receives a task to schedule, it runs its algorithm to decide where/when that task should run. 

### scx_rustland Code Structure
```
scheds/rust/scx_rustland/
├── Cargo.toml                 # Rust package definition
├── src/
│   ├── main.rs                # Main userspace implementation
│   ├── scheduler.rs           # Scheduler logic
│   ├── stats.rs               # Statistics collection
│   ├── topology.rs            # CPU topology handling
│   └── bpf/                   # BPF skeleton code
└── src/bpf/
    ├── main.bpf.c             # Minimal BPF implementation
    └── intf.h                 # Interface definitions
```

`scx_rustland` has two main components:



BPF Component (kernel space):



```c
void BPF_STRUCT_OPS(rustland_enqueue, struct task_struct *p, u64 enq_flags)
{
    // Skip scheduling the scheduler itself
    if (is_usersched_task(p))
        return;

    // Pass task information to userspace for scheduling decision
    struct queued_task_ctx *task = bpf_ringbuf_reserve(&queued, sizeof(*task), 0);
    if (!task) {
        // Fallback: direct dispatch if userspace communication fails
        scx_bpf_dsq_insert_vtime(p, SHARED_DSQ, SCX_SLICE_DFL, 0, enq_flags);
        return;
    }
    
    // Send task info to userspace
    populate_task_info(task, p, enq_flags);
    bpf_ringbuf_submit(task, 0);
}
```






### Scheduling Logic

The scheduling logic in `scx_rustland` is split across kernel (BPF) and user-space





```rust
// Main scheduler object
struct Scheduler<'a> {
    bpf: BpfScheduler<'a>,                  // BPF connector
    stats_server: StatsServer<(), Metrics>, // statistics
    task_pool: TaskTree,                    // tasks ordered by deadline
    task_map: TaskInfoMap,                  // map pids to the corresponding task information
    min_vruntime: u64,                      // Keep track of the minimum vruntime across all tasks
    init_page_faults: u64,                  // Initial page faults counter
    slice_ns: u64,                          // Default time slice (in ns)
    slice_ns_min: u64,                      // Minimum time slice (in ns)
}
```


### TBD
* rustland_select_cpu: Selects a CPU for a task
* rustland_enqueue: Handles a task becoming ready to run
* rustland_dispatch: Dispatches tasks that are ready to run
* rustland_running: Called when a task starts on its CPU
* rustland_stopping: Called when a task stops running
## scx_bpfland

`scx_bpfland` implements its scheduling logic almost entirely in BPF (Berkeley Packet Filter) code that runs in kernel space. It follows a more traditional approach where all scheduling decisions happen within the kernel:

### scx_bpfland  overiew
```
User Space: Minimal (monitoring only)
    |
    | (Minimal interaction)
    v
Kernel Space: Full scheduler implementation in BPF
    - Task selection
    - CPU assignment
    - Priority decisions
    - All scheduling algorithms
```
```
scheds/c/scx_bpfland.bpf.c        # Main BPF scheduler implementation
scheds/c/scx_bpfland.c            # Userspace loader and monitoring
scheds/include/scx_common.bpf.h   # Common BPF utilities
```

### scx_bpfland  Code Structure

## Advantages Over Traditional Schedulers

## Future integrating SCX_GOLAND with Free5GC for Data Plane Optimization

## Conclusion